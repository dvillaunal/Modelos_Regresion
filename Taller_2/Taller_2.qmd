---
lang: es
format: 
  pdf:
    documentclass: article
    geometry: margin=1in
highlight-style: github
editor: source

execute: 
  cache: true
---

\begin{titlepage}
    \begin{center}
        {\LARGE \textbf{Universidad Nacional De Colombia}} \\
        
        \vspace{4mm}
        
        {\Large \textbf{Sede Medellín}}
        \vspace{0.3cm}
        \begin{figure}[h]
            \centering
        \includegraphics[height=5cm]{logo.jpg}
        \end{figure}
    
    \vspace{1mm}
    
    {\LARGE \textbf{Facultad de Ciencias}}
    \vspace{5mm}
    
    {\Large Departamento de Estadística}
    \vspace{1.4cm}

    {\LARGE\textbf{Taller RML (Parte 2)}}
    \vspace{1.4cm}    
    
    {\Large \textbf{Daniel Felipe Villa Rengifo}}
    \vspace{4mm}
    
    {\Large \textbf{Luis David Hernández Pérez}}
    \vspace{4mm}
    
    {\Large \textbf{Juan Gabriel Carvajal Negrete}}
    \vspace{1.4cm}
    
    {\Large Modelos de Regresión}
    \vspace{1cm}
    
    {\Large\textbf{Enero, 2025}}

    \end{center}
\end{titlepage}

```{r}
#| include: false 
#| warning: false
library(car)
library(leaps)
library(pls)
library(scales)
library(olsrr)
library(rsm)
library(rgl)
library(glmnet)
library(scatterplot3d)
library(GGally)
library(corrplot)
library(xtable)
library(magrittr)
library(glue)
library(pander)
library(kableExtra)
library(knitr)
library(tidyverse)
source("https://raw.githubusercontent.com/NelfiGonzalez/Regresion-DOE/main/FuncionesdeUsuarioRLM.R")
```

Primeramente recordaremos el contexto de la base de datos y las covariables a ser incluidas para el análisis de nuestro grupo.

El conjunto de datos `boston.csv` contiene información recopilada por el Servicio del Censo de EE. UU. con respecto a la vivienda en el área de Boston,Massachusetts.

Las variables a incluir en el análisis son:

-   `CRIM`: tasa de criminalidad per cápita por ciudad.

-   `NOX`: concentración de óxidos nítricos (partes por 10 millones).

-   `RM`: Número medio de habitaciones por vivienda.

-   `AGE`: Proporción de unidades ocupadas por el propietario construidas antes de 1940.

-   `PTRATIO`: Proporción de alumnos por profesor por ciudad.

-   `LSTAT`: Porcentaje de población con nivel socio-económico bajo.

-   `MEDV`: Valor medio de las viviendas ocupadas por el propietario en \$1000.

```{r}
#| echo: false

# cargue de la base de datos

datos <- read.csv("boston.csv", header = T, sep = ",", dec = ".")
datos4 <- datos[401:500,c(1,5,6,7,11,13,14)]
```

# Punto 1

Realice diagnósticos de multicolinealidad mediante.

## a) Matriz de correlación de las variables predictoras.

```{r}
#| echo: false
#| label: tbl-cor
#| tbl-cap: "Matriz de Correlaciones"


correlaciones(datos4[,-7]) %>% kable(align = c('lc'))
```

```{r}
#| echo: false
#| label: fig-corplot
#| fig-cap: "Gráfico de Correlaciones" 

corr <- cor(datos4[,-7], method = "pearson")

corrplot(corr,
         method = "circle",            
         type = "upper",               
         tl.col = "black",             
         tl.srt = 45,                  
         number.cex = 1.2,             
         addCoef.col = "black",        
         col = colorRampPalette(c("red", "white", "blue"))(200),
         diag = FALSE)
```

De acuerdo a @tbl-cor y @fig-corplot concluimos que no hay evidencia clara de multicolinealidad severa.

## b) VIF's

```{r}
#| echo: false
#| label: tbl-vifs
#| tbl-cap: "Factores de inflación de varianza (VIF) para las variables predictoras del modelo"

# Ajuste del modelo de regresion
modelo <- lm(MEDV ~. , data = datos4)

miscoeficientes(modelo)[4] %>% kable()
```

Según la @tbl-vifs la multicolinealidad no parece ser un problema significativo en este conjunto de datos según los valores de VIF.

## c) Indice de condición

Para calcular en indice de condición tendremos en cuenta centrar los datos (Montgomeryet.al.,2021) ya que intercepto solo tendrá una interpretación útil si los valores de las variables independientes igual a cero tienen sentido en el contexto del conjunto de datos. En este caso particular, dado que algunas variables como `RM` número promedio de habitaciones), no tienen sentido físico o práctico como cero, ya que una casa no puede tener cero habitaciones, por tanto el intercepto no tendría una interpretación práctica clara.

\newpage

```{r}
#| echo: false
#| label: tbl-ic
#| tbl-cap: "Índice de condición para las variables predictoras del modelo"


multicolin(modelo, center = TRUE)[4] %>% kable(align = c('c'))
```

Según la @tbl-ic todos los valores del índice de condición están muy por debajo de 10, lo que indica que no hay problemas significativos de multicolinealidad entre las variables predictoras.

## d) Proporciones de varianza.

Para el criterio de proporciones de varianza tambien se hizo lo expuesto en el criterio anterior (Indice de condición).

```{r}
#| echo: false
#| label: tbl-pv
#| tbl-cap: "Proporciones de varianza para las variables predictoras del modelo"


multicolin(modelo, center = TRUE)[,c(3,5,6,7,8,9,10)] %>% 
  kable(align = c('ccccccc'), digits = 4)
```

Según la @tbl-pv no hay proporciones $\pi_{ij}$ altas $(> 0.5)$ para dos o más coeficientes de regresión asociados con un mismo valor propio pequeño, por tanto no hay evidencia de multicolinealidad entre las variables correspondientes a tales coeficientes.

# Punto 2

Construya modelos de regresión utilizando los métodos de selección (muestre de cada método solo la tabla de resumen de este y la tabla ANOVA y la de parámetros estimados del modelo finalmente resultante):

## a) Selección según el $R^2_\text{adj}$

```{r}
#| echo: false
#| label: tbl-Radj
#| tbl-cap: "Selección de Modelos según el Estadístico $R^2_{adj}$"

# Generar todos los modelos posibles
k <- ols_step_all_possible(modelo)

# Mejor modelo segun R2 adj
k$result %>% select(predictors, adjr) %>% arrange(-adjr) %>% head() %>% 
  kable(digits = 3, align = c('lc'))
```

\newpage

Según la @tbl-Radj podemos concluir que:

-   El modelo con las variables predictoras `CRIM`, `NOX`y `LSTAT` es el mejor segund el $R^2_{adj}$ ya que maximiza este estadístico con un número mínimo de predictores.

-   La inclusión de más variables predictoras como `PTRATIO` o `AGE` no parece justificar un mejor desempeño,ya que el $R^2_{adj}$ no mejora sustancialmente.

## b) Selección según el estadístico $C_p$.

Selección según el estadístico $C_p$.

```{r}
#| echo: false
#| label: tbl-cp
#| tbl-cap: "Selección de Modelos según el Estadístico $C_p$"


# Mejor modelo segun Cp

k$result %>% select(predictors, cp) %>% arrange(cp) %>% head() %>% 
  kable(digits = 3, align = c('lc'))
```

Según la @tbl-cp el mejor modelo, según el criterio $C_p$ es con las variables predictoras `CRIM`, `NOX`, `LSTAT`.

## c) Stepwise

```{r}
#| eval: false
#| include: false
ols_step_both_p(modelo, details = TRUE)
```

| Paso | Variable Agregada |   AIC   |   R²    | R² Ajustado |
|------|:-----------------:|:-------:|:-------:|:-----------:|
| 0    |    Base Model     | 621.008 | 0.00000 |   0.00000   |
| 1    |     LSTAT (+)     | 559.176 | 0.47182 |   0.46643   |
| 2    |      NOX (+)      | 543.986 | 0.55524 |   0.54607   |
| 3    |     CRIM (+)      | 535.404 | 0.59990 |   0.58740   |

: Resumen Stepwise {#tbl-stepwise}



| Fuente    | Suma de Cuadrados | DF  | Media Cuadrática | F     | Significancia |
|-----------|:-----------------:|:---:|:----------------:|-------|:-------------:|
| Regresión |     1679.722      |  3  |     559.907      | 47.98 |    0.0000     |
| Residual  |     1120.278      | 96  |      11.670      |       |               |
| Total     |     2800.000      | 99  |                  |       |               |

: ANOVA Stepwise {#tbl-stepwise_anova}



| Variable    | Beta    | Std. Error | Std. Beta | t      | Significancia | Inferior | Superior |
|---------|---------|---------|---------|---------|---------|---------|---------|
| (Intercept) | 40.703  | 3.495      |           | 11.646 | 0.000         | 33.765   | 47.640   |
| LSTAT       | -0.488  | 0.068      | -0.516    | -7.201 | 0.000         | -0.622   | -0.353   |
| NOX         | -23.187 | 5.663      | -0.283    | -4.094 | 0.000         | -34.428  | -11.946  |
| CRIM        | -0.095  | 0.029      | -0.225    | -3.274 | 0.001         | -0.153   | -0.037   |

: Parámetros estimados del modelo final Stepwise {#tbl-stepwise_parametros}



## d) Selección hacia adelante o \textit{forward}

```{r}
#| eval: false
#| include: false
ols_step_forward_p(modelo,p_val=0.05,details =TRUE)
```

| Paso | Variable Agregada |   AIC   |   R²    | R² Ajustado |
|------|:-----------------:|:-------:|:-------:|:-----------:|
| 0    |    Base Model     | 621.008 | 0.00000 |   0.00000   |
| 1    |       LSTAT       | 559.176 | 0.47182 |   0.46643   |
| 2    |        NOX        | 543.986 | 0.55524 |   0.54607   |
| 3    |       CRIM        | 535.404 | 0.59990 |   0.58740   |

: Resumen Forward {#tbl-forward}

| Fuente    | Suma de Cuadrados | DF  | Media Cuadrática |   F   | p-valor |
|-----------|:-----------------:|:---:|:----------------:|:-----:|:-------:|
| Regresión |     1679.722      |  3  |     559.907      | 47.98 | 0.0000  |
| Residual  |     1120.278      | 96  |      11.670      |       |         |
| Total     |     2800.000      | 99  |                  |       |         |

: ANOVA Forward {#tbl-forward_anova}

| Variable    |  Beta   | Std. Error | Std. Beta |   t    | p-valor | Inferior | Superior |
|:--------|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|
| (Intercept) | 40.703  |   3.495    |           | 11.646 |  0.000  |  33.765  |  47.640  |
| LSTAT       | -0.488  |   0.068    |  -0.516   | -7.201 |  0.000  |  -0.622  |  -0.353  |
| NOX         | -23.187 |   5.663    |  -0.283   | -4.094 |  0.000  | -34.428  | -11.946  |
| CRIM        | -0.095  |   0.029    |  -0.225   | -3.274 |  0.001  |  -0.153  |  -0.037  |

: Parámetros estimados del modelo final Forward {#tbl-forward_parametros}

## e) Selección hacia atrás o \textit{backward}

```{r}
#| eval: false
#| include: false
ols_step_backward_p(modelo,p_val=0.05,details = TRUE)
```

| Paso | Variable Eliminada |   AIC   |   R²    | R² Ajustado |
|------|:------------------:|:-------:|:-------:|:-----------:|
| 0    |     Full Model     | 540.494 | 0.60353 |   0.57795   |
| 1    |         RM         | 538.608 | 0.60307 |   0.58196   |
| 2    |        AGE         | 536.769 | 0.60243 |   0.58569   |
| 3    |      PTRATIO       | 535.404 | 0.59990 |   0.58740   |

: Resumen Backward {#tbl-backward}

| Fuente    | Suma de Cuadrados | DF  | Media Cuadrática | F     | p-valor |
|:----------|-------------------|-----|------------------|-------|---------|
| Regresión | 1679.722          | 3   | 559.907          | 47.98 | 0.0000  |
| Residual  | 1120.278          | 96  | 11.670           |       |         |
| Total     | 2800.000          | 99  |                  |       |         |

: ANOVA Backward {#tbl-backward_anova}

| Variable    |  Beta   | Std. Error | Std. Beta |   t    | p-valor | Inferior | Superior |
|:--------|:-------:|:-------:|---------|:-------:|:-------:|:-------:|:-------:|
| (Intercept) | 40.703  |   3.495    |           | 11.646 |  0.000  |  33.765  |  47.640  |
| CRIM        | -0.095  |   0.029    | -0.225    | -3.274 |  0.001  |  -0.153  |  -0.037  |
| NOX         | -23.187 |   5.663    | -0.283    | -4.094 |  0.000  | -34.428  | -11.946  |
| LSTAT       | -0.488  |   0.068    | -0.516    | -7.201 |  0.000  |  -0.622  |  -0.353  |

: Parámetros estimados del modelo final Backward {#tbl-backward_parametros}

De los tres métodos de selección podemos concluir que:

- Los tres métodos seleccionaron las mismas tres variables (LSTAT, NOX y CRIM), lo que sugiere que estas son las más relevantes para predecir MEDV.

-   Los tres métodos seleccionaron el mismo modelo final, lo que confirma la estabilidad y robustez del proceso de selección.

-   El modelo final es significativo y bien ajustado, con un $R^2$ ajustado de 0.587 y un $AIC$ mínimo de 535.404.

-   Las variables clave (LSTAT, NOX y CRIM) tienen un impacto negativo en MEDV, indicando que el nivel socio-económico, la contaminación y la criminalidad son determinantes en los precios de la vivienda. Dado el desempeño similar de los métodos, cualquiera de ellos es válido, pero **Stepwise** combinado puede considerarse el más flexible al evaluar tanto adiciones como eliminaciones de variables en el proceso.

# Punto 3

Realice el ajuste utilizando los métodos RR y LASSO. Compare los resultados y comente.

## Regresión Ridge

```{r}
# Creando matriz de diseño

X <- as.matrix(model.matrix(modelo))[,-1]
y <- datos4$MEDV

# Ajuste del modelo de regresion ridge

model_ridge <- glmnet(X, y, alpha = 0)
```

Para identificar el valor de **kappa** que da lugar al mejor modelo,recurriremos a validación cruzada con la función `cv.glmnet()`.

```{r}
#| echo: false


set.seed(21)
cv_model_rr <- cv.glmnet(X, y, alpha = 0) # encontrado el mejor kappa
```

```{r}
#| echo: false

# Mejor valor lambda encontrado
# =========================================================================
paste("Mejor valor de kappa encontrado:", cv_model_rr$lambda.min)
```

Ahora ajustamos el modelo de regresión ridge nuevamente con el valor de kappa optimo.

\newpage

```{r}
#| eval: false
#| include: false
# Mejor modelo con el kappa óptimo
mejor_model_rr <- glmnet(X, y, alpha = 0,lambda = cv_model_rr$lambda.min)
coef.glmnet(mejor_model_rr)
```

| Variable    | Coeficientes |
|-------------|--------------|
| (Intercept) | 55.64931433  |
| CRIM        | -0.07570871  |
| NOX         | -19.09952447 |
| RM          | 0.55650949   |
| AGE         | -0.03113410  |
| PTRATIO     | -1.03489490  |
| LSTAT       | -0.36365557  |

: Coeficientes regresión ridge con el mejor kappa {#tbl-coef_rr}

```{r}
#| echo: false

# Predicciones modelo de regresion ridge
predicciones_rr <- predict(model_ridge, 
                           s = cv_model_rr$lambda.min,
                           newx = X)
```

```{r}
#| echo: false

# MSE mejor modelo de regresion ridge
mse_rr <- mean((predicciones_rr - y)^2)
paste("Error (mse) mejor modelo ridge:", mse_rr)
```

## Regresión Lasso

```{r}
# Ajuste del modelo de regresion lasso
model_lasso <- glmnet(X, y, alpha = 1)
```

Para identificar el valor de **kappa** que da lugar al mejor modelo,recurriremos a validación cruzada con la función `cv.glmnet()`.

```{r}
#| echo: false
set.seed(64)
cv_model_lasso <- cv.glmnet(X, y, alpha = 1) # encontrado el mejor kappa
```

```{r}
#| echo: false

# Mejor valor kappa encontrado
paste("Mejor valor de kappa encontrado:", cv_model_lasso$lambda.min)
```

Ahora ajustamos el modelo de regresión lasso nuevamente con el valor de kappa optimo .

```{r}
#| eval: false
#| include: false


# Mejor modelo con el kappa óptimo 

mejor_model_lasso <- glmnet(X, y, alpha = 1,lambda = cv_model$lambda.min)
coef.glmnet(mejor_model_lasso)
```

| Variable    | Coeficientes |
|-------------|--------------|
| (Intercept) | 29.47864426  |
| CRIM        | -0.02935753  |
| NOX         | -10.47212401 |
| RM          | .            |
| AGE         | .            |
| PTRATIO     | .            |
| LSTAT       | -0.36882565  |

: Coeficientes regresión lasso con el mejor kappa {#tbl-coef_lasso}

```{r}
#| echo: false

# Predicciones mejor modelo de regresion lasso
predicciones_lasso <- predict(model_lasso, 
                              s =cv_model_lasso$lambda.min,
                              newx = X)
```

```{r}
#| echo: false

# MSE modelo de regresion lasso

mse_lasso <- mean((predicciones_lasso - y)^2)
paste("Error (mse) mejor modelo lasso:", mse_lasso)
```

De la @tbl-coef_rr y @tbl-coef_lasso que representa los coeficientes de los modelos con el mejor valor optimo de kappa podemos notar que:

-   Ridge mantiene todas las variables en el modelo, pero penaliza sus coeficientes, reduciendo su magnitud.

-   Lasso seleccionó solo tres variables (CRIM, NOX, LSTAT) como las más relevantes, eliminando las demás.

\newpage

Después de ajustar los modelos de Ridge y LASSO utilizando los valores óptimos de Kappa y realizar predicciones, calculamos los errores medios cuadrados (MSE) para cada modelo.

| Modelo | MSE (Error Medio Cuadrático) |
|--------|:----------------------------:|
| Ridge  |           11.4030            |
| LASSO  |           11.3078            |



: MSE mejores modelos Ridge y Lasso {#tbl-mse}


De la @tbl-mse podemos decir que ambos modelos tienen errores similares, el modelo LASSO es ligeramente superior en términos de precisión predictiva debido a su menor MSE, la diferencia en el MSE entre ambos métodos es pequeña (~0.1), lo que sugiere que ambos modelos son adecuados para el problema (predicción).





# Punto 4

Realice el ajuste PCR y comente.


Primera mente aplicamos analisis de componetes principales a las variables predictoras.

```{r}
#| echo: false
pca <- prcomp(datos4[,-7], scale. = TRUE)
pca %>% summary()
```

De acuedo a la salidad anterior basado en la proporción acumulada de varianza, 3 componentes principales son suficientes para capturar la mayor parte de la información relevante (77.2%).



Ahora ajustemos sel modelo PCR realizando validación cruzada para elegir el número óptimo de componentes principales.


```{r}
set.seed(6475)
pcr_model <- pcr(MEDV ~ ., data = datos4, scale = TRUE, validation = "CV")
summary(pcr_model)
```

De la salida anterior podemos concluir que:

- El error predicho disminuye significativamente hasta los 3 componentes principales y luego se estabiliza.

- 3 componentes principales parece ser el número óptimo, ya que tiene el menor RMSEP ajustado (adjCV) de 3.632.

- Los 3 primeros componentes principales explican el 77.22% de la varianza en las variables predictoras $X$ y el 57.04% de la varianza en MEDV.

- Agregar más componentes (4, 5, o 6) no mejora significativamente la varianza explicada en MEDV.



Después de seleccionar el número óptimo de componentes, realizaremos predicciones y calcularemos el error medio cuadrático (MSE).


```{r}
# Predicciones utilizando los componentes seleccionados
predicciones <- predict(pcr_model, ncomp = 3, newdata = datos4)

# Calcular el MSE
mse_pcr <- mean((predicciones - datos4$MEDV)^2)
print(paste("MSE del modelo PCR:", mse_pcr))
```

El MSE de PCR (12.03) es ligeramente mayor que el de Ridge (11.403) y LASSO (11.308).
Esto indica que Ridge y LASSO son marginalmente más precisos en este caso específico

